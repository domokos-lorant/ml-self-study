{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196b4e08",
   "metadata": {},
   "source": [
    "Based on https://github.com/ed-donner/llm_engineering/blob/main/week1/day2%20EXERCISE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c43222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3204744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"phi4-mini\"\n",
    "MODEL_RAW = \"phi4-mini-raw\"\n",
    "MODEL_GPT_OSS = \"gpt-oss:20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14568984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4bd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8431610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 3c168af1dea0: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.5 GB                         \u001b[K\n",
      "pulling 813f53fdc6e5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  655 B                         \u001b[K\n",
      "pulling fa8235e5b48f: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 KB                         \u001b[K\n",
      "pulling 8c2539a423c4: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  411 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull phi4-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb86d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Hope that tickles your funny bone. Keep smiling and laughing is just another way to stay healthy (and happy)! üòÑ\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a84a53d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one for you: Why don't scientists trust atoms? Because they make up everything! üòÑ\n",
      "\n",
      "\n",
      "Remember that humor can be subjective and sensitive to context. If there's anything specific you're looking into or need information on that's related to telling jokes effectively in particular contexts (like professional settings), please let me know so I could assist better!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"<|system|>you are a helpful assistant\n",
    "              <|user|>Tell me a joke!\n",
    "              <|end|><|assistant|>\"\"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "payload_raw = {\n",
    "        \"model\": MODEL_RAW,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload_raw, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b0517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one for you: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! üòÑ\n",
      "\n",
      "Remember that jokes can be subjective and it's important to share humor respectfully. If there's anything else you'd like‚Äîwhether fun or informative‚ÄîI‚Äôm here to help with it as well!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke! Try to be extra funny and original!\"}\n",
    "]\n",
    "\n",
    "payload_gpt = {\n",
    "        \"model\": MODEL_GPT_OSS,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload_raw, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
