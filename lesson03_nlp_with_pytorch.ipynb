{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d516f2",
   "metadata": {},
   "source": [
    "# NLP with PyTorch + torchtext â€” example\n",
    "\n",
    "This notebook contains an independent implementation inspired by the DataCamp article \"NLP with PyTorch: A comprehensive guide\" (https://www.datacamp.com/tutorial/nlp-with-pytorch-a-comprehensive-guide). The code here is an original implementation using torch + torchtext to train a simple text classifier on the AG_NEWS dataset.  While some of the necessary pytorch tools are now deprecated, this notebook will give you a practical introduction into the basics of training a model using pytorch.\n",
    "\n",
    "Attribution: concepts and high-level approach are inspired by the DataCamp article, but all code in this notebook is independently written and adapted for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4ce1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.0+cu121\n",
      "torchtext version: 0.18.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Optional: try to import torch and torchtext; if missing, attempt to install minimal packages\n",
    "# Note: installing torch via pip can be large and may require a specific wheel for CUDA/OS.\n",
    "# If pip install fails, install PyTorch following official instructions for your environment.\n",
    "\n",
    "import sys\n",
    "try:\n",
    "    import torch, torchtext\n",
    "    print(\"torch version:\", torch.__version__)\n",
    "    print(\"torchtext version:\", torchtext.__version__)\n",
    "except Exception as e:\n",
    "    print(\"torch or torchtext import failed:\\n\", e)\n",
    "    print(\"Attempting to install torchtext (and torch if feasible). This may take a while.\")\n",
    "    import subprocess\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"]\n",
    "    subprocess.run(cmd, check=False)\n",
    "    # Try to install torchtext and a CPU-compatible torch if not present\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"torchtext==0.18.0\", \"torch==2.3.0\", \"torchdata==0.9.0\", \"portalocker>=2.0.0\"], check=False)    \n",
    "    print(\"Install attempts finished. Restart the kernel or re-run this cell and imports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515acb9",
   "metadata": {},
   "source": [
    "This cell imports required Python libraries and sets the compute device (CPU or CUDA). It includes torch, torchtext, and utilities used later for building the dataset, model, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsim/github/ml-self-study/.venv/lib/python3.12/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/rsim/github/ml-self-study/.venv/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/rsim/github/ml-self-study/.venv/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/rsim/github/ml-self-study/.venv/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb553",
   "metadata": {},
   "source": [
    "This cell prepares tokenization and builds the vocabulary from the training split of the AG_NEWS dataset. It also defines small helper pipelines to convert raw text and labels into numerical tensors used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0cf08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsim/github/ml-self-study/.venv/lib/python3.12/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 95811\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer, vocabulary and label mapping\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')  # small, effective tokenizer\n",
    "\n",
    "# Build vocabulary from the training splits\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Use streaming iterator to avoid loading everything in memory\n",
    "train_iter = AG_NEWS(split='train')\n",
    "# build vocab\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "num_tokens = len(vocab)\n",
    "print('Vocab size:', num_tokens)\n",
    "\n",
    "# Helper to map raw text -> tensor of token IDs\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  # AG_NEWS labels are 1..4, convert to 0..3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ebd2b0",
   "metadata": {},
   "source": [
    "This cell defines the collate function used by the PyTorch DataLoader. The collate function converts a batch of raw examples into a packed format (flattened token tensor + offsets) suitable for an EmbeddingBag-based model, and converts labels to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dede681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text size torch.Size([338])\n",
      "offsets tensor([  0,  29,  71, 111, 151, 194, 242, 289])\n",
      "labels tensor([2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Collate function for DataLoader using EmbeddingBag-style batching\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "    offsets = [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed = torch.tensor(text_pipeline(_text), dtype=torch.long)\n",
    "        text_list.append(processed)\n",
    "        offsets.append(processed.size(0))\n",
    "    labels = torch.tensor(label_list, dtype=torch.long)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text_list)\n",
    "    return text.to(device), offsets.to(device), labels.to(device)\n",
    "\n",
    "# Quick test of the collate with a tiny subset\n",
    "small_train = list(AG_NEWS(split='train'))[:8]\n",
    "text, offsets, labels = collate_batch(small_train)\n",
    "print('text size', text.size())\n",
    "print('offsets', offsets)\n",
    "print('labels', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163895a",
   "metadata": {},
   "source": [
    "This cell shows how the dataset is materialized and wrapped into DataLoader objects for training and testing. It converts streaming iterators to lists and configures batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c31207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1875 Test batches: 119\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for train and test\n",
    "batch_size = 64\n",
    "train_dataset = AG_NEWS(split='train')\n",
    "test_dataset = AG_NEWS(split='test')\n",
    "\n",
    "# DataLoaders: wrap the iterators with list to avoid streaming behavior in DataLoader\n",
    "train_list = list(train_dataset)\n",
    "test_list = list(test_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_list, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_list, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print('Train batches:', len(train_dataloader), 'Test batches:', len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50409e4",
   "metadata": {},
   "source": [
    "This cell defines a compact text classification model using an EmbeddingBag layer followed by a linear classifier. EmbeddingBag performs a sum/mean aggregation of token embeddings per example, which works well for short text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fe1dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(95811, 64, mode='mean')\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a simple text classifier using EmbeddingBag\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "num_class = 4\n",
    "embed_dim = 64\n",
    "model = TextClassificationModel(num_tokens, embed_dim, num_class).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3875c62",
   "metadata": {},
   "source": [
    "This cell contains the loss function, optimizer, and the training/validation loops. The training loop iterates over batches, computes loss and backpropagates, while the evaluation loop measures accuracy on a held-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c062bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation utilities\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5.0)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, offsets, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = output.argmax(1)\n",
    "        total_acc += (pred == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "\n",
    "    return total_acc / total_count\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, offsets, labels) in enumerate(dataloader):\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            pred = output.argmax(1)\n",
    "            total_acc += (pred == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4cd8e",
   "metadata": {},
   "source": [
    "Run a short training cycle using the training and evaluation utilities defined above. This cell performs a quick sanity run for a small number of epochs to verify the pipeline is working end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a2f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train acc: 0.9056, Test acc: 0.8964\n",
      "Epoch: 2, Train acc: 0.9173, Test acc: 0.9071\n",
      "Done training (short run).\n"
     ]
    }
   ],
   "source": [
    "# Run a short training loop (1-3 epochs) to verify everything works\n",
    "\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    train_acc = train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    test_acc = evaluate(model, test_dataloader, criterion)\n",
    "    print(f'Epoch: {epoch+1}, Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}')\n",
    "\n",
    "print('Done training (short run).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c95f33",
   "metadata": {},
   "source": [
    "Notes / next steps\n",
    "\n",
    "- This example builds a small vocabulary and trains a simple EmbeddingBag model for AG_NEWS.\n",
    "- Suggested exercise: modify the training loop to run for 10 epochs and plot the training and test accuracy versus epoch.\n",
    "- For better performance:\n",
    "  - Use pretrained embeddings or larger embed_dim.\n",
    "  - Use regularization, LR scheduling, or a different optimizer (Adam).\n",
    "  - Use text preprocessing: lowercasing, stopword filtering, subword tokenization.\n",
    "  - Explore transformer-based models with Hugging Face for state-of-the-art performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
