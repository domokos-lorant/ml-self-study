{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2a9bf9",
   "metadata": {},
   "source": [
    "# Hypothesis testing \n",
    "\n",
    "This notebook is based on recipes in https://github.com/galenwilkerson/hypothesis-testing but adds a hands-on, first-principles example and faster numpy-based approaches. We'll test a simple one-sided hypothesis about a thumbs-up rate for AI conversational output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32e85d",
   "metadata": {},
   "source": [
    "We'll work through a detailed example: users provide \"thumbs‑up\" responses to Copilot output. We'll:\n",
    "\n",
    "1. Compute a p-value from first principles (binomial probability) for a simple one-sided test (H0: p = 0.01 vs H1: p > 0.01).\n",
    "2. Compare an analytic normal approximation.\n",
    "3. Show a fast Monte Carlo / numpy simulation approach.\n",
    "\n",
    "We'll pick a sample size and observed successes that give a p-value near the 1% level (so it's a demonstrably significant result at the 95% level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd67a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.rcParams['figure.dpi'] = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23569d",
   "metadata": {},
   "source": [
    "## First principles: binomial test for thumbs-up rate\n",
    "\n",
    "Setup: we have a control (baseline) flight and treatment flight of Copilot. We want to determine whether the treatment flight is likely to yield more thumbs-up responses.\n",
    "- Null hypothesis H0: the true thumbs-up rate p = 0.01 (baseline)\n",
    "- Alternative H1: p > 0.01 (one-sided — we want to detect a higher thumbs-up rate)\n",
    "\n",
    "We collect n observations and count k thumbs-up responses. From first principles, under H0 the number of thumbs-ups is Binomial(n, p0). The one-sided p-value is:\n",
    "\n",
    "$$\n",
    "\\mathrm{p\\text{-}value} = P_{X\\sim\\mathrm{Binomial}(n,p_0)}(X \\ge k)\n",
    "= \\sum_{i=k}^n \\binom{n}{i} p_0^{i} (1 - p_0)^{n-i}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b83d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed thumbs-up: 18/1000 = 1.800%\n",
      "Exact one-sided p-value P(X >= 18 | p0=0.01) = 1.383258e-02\n",
      "Result: reject H0 at 95% significance (one-sided)\n"
     ]
    }
   ],
   "source": [
    "# Example parameters \n",
    "p0 = 0.01   # baseline thumbs-up rate under H0\n",
    "n = 1000    # treatment sample size\n",
    "k = 18      # observed thumbs-up count (choose to target p ~ 0.01)\n",
    "\n",
    "# compute exact one-sided p-value P(X >= k | p0) using log pmf to avoid overflow\n",
    "\n",
    "def log_binom_pmf(n, k, p):\n",
    "    \"\"\"\n",
    "    Compute the natural logarithm of the binomial probability mass function:\n",
    "\n",
    "        log P(X = k) = log( C(n, k) * p**k * (1-p)**(n-k) )\n",
    "\n",
    "    This function returns the log PMF (natural log) and uses the log-gamma\n",
    "    function (math.lgamma) to compute log(C(n,k)) in a numerically stable way,\n",
    "    which is important for large n.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of Bernoulli trials.\n",
    "    k : int\n",
    "        Number of observed successes.\n",
    "    p : float\n",
    "        Success probability (0 < p < 1) under the null hypothesis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The natural log of the binomial PMF at k.\n",
    "    \"\"\"\n",
    "    return math.lgamma(n+1) - math.lgamma(k+1) - math.lgamma(n-k+1) + k*math.log(p) + (n-k)*math.log(1-p)\n",
    "\n",
    "ks = np.arange(k, n+1)\n",
    "logpmfs = np.array([log_binom_pmf(n, i, p0) for i in ks])\n",
    "pmfs = np.exp(logpmfs)\n",
    "p_value_exact = pmfs.sum()\n",
    "\n",
    "print(f\"Observed thumbs-up: {k}/{n} = {k/n:.3%}\")\n",
    "print(f\"Exact one-sided p-value P(X >= {k} | p0={p0}) = {p_value_exact:.6e}\")\n",
    "\n",
    "if p_value_exact < 0.05:\n",
    "    print(\"Result: reject H0 at 95% significance (one-sided)\")\n",
    "else:\n",
    "    print(\"Result: fail to reject H0 at 95% significance (one-sided)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977d04e",
   "metadata": {},
   "source": [
    "### What the Monte Carlo cell does\n",
    "\n",
    "This cell runs a Monte Carlo simulation to estimate the one-sided p-value under the null hypothesis H0 (`p = p0`).\n",
    "\n",
    "- It draws `trials` independent samples from a Binomial(`n`, `p0`) null distribution using `np.random.binomial`.\n",
    "- It computes the empirical p-value as the fraction of simulated trials with counts ≥ the observed `k` (i.e., an estimate of P(X ≥ k) under H0).\n",
    "- It prints the estimated p-value and plots a histogram of the simulated null distribution; the observed `k` is marked with a red dashed line to show how extreme the observation is.\n",
    "\n",
    "Notes:\n",
    "- Monte Carlo is useful when exact calculations are expensive or when SciPy is not available.\n",
    "- Monte Carlo sampling error scales roughly as 1/√(trials); increase `trials` for more precise estimates.\n",
    "- A fixed random seed (set earlier) ensures reproducible simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal (Gaussian) approximation with continuity correction\n",
    "mu = n * p0\n",
    "sigma = math.sqrt(n * p0 * (1 - p0))\n",
    "\n",
    "# continuity correction: subtract 0.5 from k\n",
    "z = (k - 0.5 - mu) / sigma\n",
    "p_value_normal = 0.5 * math.erfc(z / math.sqrt(2))\n",
    "\n",
    "print(f\"Normal approx (continuity-corrected) one-sided p-value = {p_value_normal:.6e}\")\n",
    "\n",
    "print(f\"Parameters: mu={mu:.3f}, sigma={sigma:.3f}, z={z:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo (numpy) simulation to estimate p-value\n",
    "trials = 100000\n",
    "sim = np.random.binomial(n, p0, size=trials)\n",
    "p_value_mc = (sim >= k).mean()\n",
    "\n",
    "print(f\"Monte Carlo p-value estimate (n_runs={trials}) = {p_value_mc:.6e}\")\n",
    "\n",
    "# quick sanity: show a small histogram of simulated counts\n",
    "plt.hist(sim, bins=30, alpha=0.6)\n",
    "plt.axvline(k, color='red', linestyle='--', label=f'observed k={k}')\n",
    "plt.title('Monte Carlo null distribution (Binomial(n,p0))')\n",
    "plt.xlabel('number of thumbs-up')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850ff7f",
   "metadata": {},
   "source": [
    "## Faster / numpy-based approaches\n",
    "\n",
    "After demonstrating the calculation from first principles, here are faster ways to get the same result:\n",
    "- Use a vectorized log-PMF approach (compute log PMF for the relevant range and exponentiate), which avoids repeated high-level combinatorics and is numerically stable when implemented with log-gamma.\n",
    "- Use `np.random.binomial` for a fast Monte Carlo estimate (often the simplest when SciPy is unavailable).\n",
    "- If you have SciPy available, `scipy.stats.binom.sf(k-1, n, p0)` computes the exact survival function efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9636795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized log-PMF approach (deterministic exact result)\n",
    "ks_all = np.arange(0, n+1)\n",
    "logpmfs_all = np.array([math.lgamma(n+1) - math.lgamma(i+1) - math.lgamma(n-i+1) + i*math.log(p0) + (n-i)*math.log(1-p0) for i in ks_all])\n",
    "pmfs_all = np.exp(logpmfs_all)\n",
    "\n",
    "# survival function P(X >= k)\n",
    "p_value_vec = pmfs_all[k:].sum()\n",
    "print(f\"Vectorized exact p-value = {p_value_vec:.6e}\")\n",
    "\n",
    "# sanity check: compare values\n",
    "print(f\"Exact (sum over tail) = {p_value_exact:.6e}\")\n",
    "print(f\"Normal approx               = {p_value_normal:.6e}\")\n",
    "print(f\"Monte Carlo estimate        = {p_value_mc:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a7bdb",
   "metadata": {},
   "source": [
    "## Conclusion & exercises\n",
    "\n",
    "With the chosen parameters (n=1000, k=18) the one-sided p-value under H0: p=0.01 is well below 0.05 (and near 0.01), so the observed rate would be considered significant at the 95% level.\n",
    "\n",
    "Exercises:\n",
    "- Vary `n` and `k` to see how sample size affects detectability.\n",
    "- Try a two-sided test and compare thresholds.\n",
    "- Replace the Monte Carlo simulation with `scipy.stats.binom.sf` if SciPy is available and compare runtimes.\n",
    "- Apply the same workflow to a real dataset: collect N user responses, count successes, and report a p-value with a short write-up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
