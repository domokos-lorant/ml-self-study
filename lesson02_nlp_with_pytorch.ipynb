{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d516f2",
   "metadata": {},
   "source": [
    "# NLP with PyTorch + torchtext — example\n",
    "\n",
    "This notebook contains an independent implementation inspired by the DataCamp article \"NLP with PyTorch: A comprehensive guide\" (https://www.datacamp.com/tutorial/nlp-with-pytorch-a-comprehensive-guide). The code here is an original implementation using torch + torchtext to train a simple neural network classifier on the AG_NEWS dataset.  While some of the necessary pytorch tools are now deprecated, this notebook will give you a practical introduction into the basics of training a model using pytorch.\n",
    "\n",
    "**Note:** this notebook depends on torch==2.3.0 and torchtext==0.18.0.  Later versions of torch may be incompatible with torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4ce1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.0+cu121\n",
      "torchtext version: 0.18.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Optional: try to import torch and torchtext; if missing, attempt to install minimal packages\n",
    "\n",
    "import sys\n",
    "try:\n",
    "    import torch, torchtext\n",
    "    print(\"torch version:\", torch.__version__)\n",
    "    print(\"torchtext version:\", torchtext.__version__)\n",
    "except Exception as e:\n",
    "    print(\"torch or torchtext import failed:\\n\", e)\n",
    "    print(\"Attempting to install torchtext (and torch if feasible). This may take a while.\")\n",
    "    import subprocess\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"]\n",
    "    subprocess.run(cmd, check=False)\n",
    "    # Try to install torchtext and a CPU-compatible torch if not present\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"torchtext==0.18.0\", \"torch==2.3.0\", \"torchdata==0.9.0\", \"portalocker>=2.0.0\"], check=False)    \n",
    "    print(\"Install attempts finished. Restart the kernel or re-run this cell and imports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515acb9",
   "metadata": {},
   "source": [
    "Import required Python libraries and set the compute device (CPU or CUDA). These include torch, torchtext, and utilities used later for building the dataset, model, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae1df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb553",
   "metadata": {},
   "source": [
    "Prepare tokenization and build the vocabulary from the training split of the AG_NEWS dataset. This cell also defines small helper pipelines to convert raw text and labels into numerical tensors used by the model. You may safely ignore any deprecation warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d0cf08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 95811\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer, vocabulary and label mapping\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')  # small, effective tokenizer\n",
    "\n",
    "# Build vocabulary from the training splits\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Use streaming iterator to avoid loading everything in memory\n",
    "train_iter = AG_NEWS(split='train')\n",
    "# build vocab\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "num_tokens = len(vocab)\n",
    "print('Vocab size:', num_tokens)\n",
    "\n",
    "# Helper to map raw text -> tensor of token IDs\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  # AG_NEWS labels are 1..4, convert to 0..3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ebd2b0",
   "metadata": {},
   "source": [
    "Define the collate function used by the PyTorch DataLoader. The collate function converts a batch of raw examples into a packed format (flattened token tensor + offsets) suitable for an EmbeddingBag-based model, and converts labels to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dede681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text size torch.Size([338])\n",
      "offsets tensor([  0,  29,  71, 111, 151, 194, 242, 289])\n",
      "labels tensor([2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Collate function for DataLoader using EmbeddingBag-style batching\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Collate a list of (label, text) samples into the flattened tensors\n",
    "    expected by `nn.EmbeddingBag`.\n",
    "\n",
    "    Args:\n",
    "        batch: list of tuples (label, text)\n",
    "               - label: original dataset label (e.g., 1..4 for AG_NEWS)\n",
    "               - text: raw string text\n",
    "\n",
    "    Returns:\n",
    "        text: 1D LongTensor containing concatenated token IDs for the whole batch\n",
    "        offsets: 1D LongTensor where each element is the starting index of the\n",
    "                 corresponding sample in `text` (used by EmbeddingBag)\n",
    "        labels: 1D LongTensor of zero-based class indices\n",
    "\n",
    "    Conceptual worked example (numbers):\n",
    "        Suppose the tokenized & ID-mapped samples are:\n",
    "          sample0 -> [10, 23, 5]   (length 3)\n",
    "          sample1 -> [7, 12]       (length 2)\n",
    "          sample2 -> [4, 9, 11, 2] (length 4)\n",
    "\n",
    "        Building offsets as lengths yields offsets = [0, 3, 2, 4] (the last\n",
    "        entry is the last sample length appended as a placeholder). We drop\n",
    "        the trailing placeholder then cumsum:\n",
    "          offsets[:-1] = [0, 3, 2]\n",
    "          cumsum -> [0, 3, 5]\n",
    "\n",
    "        Flattened text = [10,23,5, 7,12, 4,9,11,2]\n",
    "        offsets = [0, 3, 5]\n",
    "        Each sample's span within flattened text:\n",
    "          sample0 -> flattened[0:3]  == [10,23,5]\n",
    "          sample1 -> flattened[3:5]  == [7,12]\n",
    "          sample2 -> flattened[5:9]  == [4,9,11,2]\n",
    "\n",
    "    Important notes:\n",
    "      - `offsets` length == batch_size. Each offsets[i] is the starting index\n",
    "        in `text` for sample i.\n",
    "      - EmbeddingBag takes (text, offsets) and for each offset returns an\n",
    "        aggregated embedding for the corresponding segment (e.g. mean or sum).\n",
    "      - When using DataLoader with `num_workers>0` do NOT move tensors to CUDA\n",
    "        inside `collate_fn`. Collate runs in worker processes and CUDA calls\n",
    "        there will fail or be inefficient. Instead, move tensors to `device`\n",
    "        inside the training loop (see recommended pattern below).\n",
    "    \"\"\"\n",
    "    label_list = []   # will hold integer class indices for each sample\n",
    "    text_list = []    # temporary list of 1D tensors (token ID tensors)\n",
    "    offsets = [0]     # start positions; initial 0 for the first sample\n",
    "\n",
    "    for (_label, _text) in batch:\n",
    "        # Convert dataset label to zero-based index with the shared label_pipeline\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # Tokenize + map tokens -> IDs, then convert to a torch tensor\n",
    "        # text_pipeline returns a sequence (list) of ints for the sample\n",
    "        processed = torch.tensor(text_pipeline(_text), dtype=torch.long)\n",
    "        text_list.append(processed)\n",
    "\n",
    "        # Store the length of the current sample; later we convert lengths\n",
    "        # into start indices using cumulative sum.\n",
    "        offsets.append(processed.size(0))\n",
    "\n",
    "    # Convert lists into tensors expected by the model / loss function\n",
    "    labels = torch.tensor(label_list, dtype=torch.long)  # shape: (batch_size,)\n",
    "\n",
    "    # offsets[:-1] discards the trailing length placeholder; cumsum yields\n",
    "    # start positions for each sample in the flattened `text` tensor.\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "\n",
    "    # Concatenate all per-sample token-ID tensors into a single 1D tensor.\n",
    "    # Example shape: (total_tokens_in_batch,)\n",
    "    text = torch.cat(text_list)\n",
    "\n",
    "    # Move tensors to the configured device. Note: if DataLoader uses\n",
    "    # num_workers>0 and CUDA, moving tensors to GPU inside collate_fn is\n",
    "    # not recommended — prefer moving to device in the training loop.\n",
    "    return text.to(device), offsets.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "# Quick test of the collate with a tiny subset\n",
    "# This runs a small sanity check to show shapes and example values.\n",
    "small_train = list(AG_NEWS(split='train'))[:8]\n",
    "text, offsets, labels = collate_batch(small_train)\n",
    "print('text size', text.size())      # 1D size == total number of tokens in the 8 examples\n",
    "print('offsets', offsets)            # start index per sample (length == batch size)\n",
    "print('labels', labels)              # zero-based class indices for the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163895a",
   "metadata": {},
   "source": [
    "The dataset is materialized and wrapped into DataLoader objects for training and testing. Streaming iterators are converted to lists and batching and shuffling are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c31207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1875 Test batches: 119\n",
      "\n",
      "Example batch shapes:\n",
      " text (flattened token IDs) shape: torch.Size([2710])\n",
      " offsets shape (one start index per sample): torch.Size([64])\n",
      " labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for train and test\n",
    "#\n",
    "# Note: `AG_NEWS(split='train')` returns a streaming iterator (an iterable\n",
    "# datapipes object). Converting it to `list(...)` materializes the entire\n",
    "# dataset in memory which is convenient for small/medium datasets. If you\n",
    "# are working with very large datasets or want streaming behavior, consider\n",
    "# using an IterableDataset or the torchdata streaming tools instead.\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Obtain dataset iterators (streaming)\n",
    "train_dataset = AG_NEWS(split='train')\n",
    "test_dataset = AG_NEWS(split='test')\n",
    "\n",
    "# Materialize into lists so DataLoader can index / iterate multiple times.\n",
    "# Memory tradeoff: AG_NEWS is moderate in size (commonly used training set\n",
    "# sizes are on the order of 100k examples), so this is usually fine for\n",
    "# local experiments. If memory is constrained, recreate iterators each epoch\n",
    "# or use streaming APIs.\n",
    "train_list = list(train_dataset)\n",
    "test_list = list(test_dataset)\n",
    "\n",
    "# Create DataLoader objects. We pass our `collate_batch` function which\n",
    "# converts a list of raw examples into (text, offsets, labels). If you set\n",
    "# `num_workers>0`, ensure `collate_batch` does not call CUDA APIs (move to\n",
    "# device in the training loop instead).\n",
    "train_dataloader = DataLoader(\n",
    "    train_list,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    # Recommended extras:\n",
    "    # num_workers=4,         # parallelize loading/collation on CPU\n",
    "    # pin_memory=True,       # useful when moving CPU tensors to GPU\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_list,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "print('Train batches:', len(train_dataloader), 'Test batches:', len(test_dataloader))\n",
    "\n",
    "# Quick runtime example: inspect one batch to see shapes and types. This is\n",
    "# useful to validate that collate is producing the expected tensors.\n",
    "# WARNING: if `collate_batch` moves tensors to `device` and `device` is CUDA,\n",
    "# the example below will place them on GPU — which is fine for a quick check.\n",
    "batch_iter = iter(train_dataloader)\n",
    "text_b, offsets_b, labels_b = next(batch_iter)\n",
    "print('\\nExample batch shapes:')\n",
    "print(' text (flattened token IDs) shape:', text_b.size())\n",
    "print(' offsets shape (one start index per sample):', offsets_b.size())\n",
    "print(' labels shape:', labels_b.size())\n",
    "\n",
    "# Example: if you prefer to keep tensors on CPU in collate, then move to\n",
    "# device in the training loop as shown here (preferred when using workers):\n",
    "#\n",
    "# for text, offsets, labels in train_dataloader:\n",
    "#     text = text.to(device)\n",
    "#     offsets = offsets.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     output = model(text, offsets)\n",
    "#     ...\n",
    "\n",
    "# Alternative: for very large datasets avoid list(...) materialization and\n",
    "# implement an IterableDataset that yields tokenized samples on the fly,\n",
    "# or use torchdata's streaming datasets when available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50409e4",
   "metadata": {},
   "source": [
    "Define a compact text classification model using an EmbeddingBag layer followed by a linear classifier. EmbeddingBag performs a sum/mean aggregation of token embeddings per example, which works well for short text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5fe1dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demo forward pass logits shape: torch.Size([2, 4])\n",
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(95811, 64, mode='mean')\n",
      "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a simple text classifier using EmbeddingBag\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact text classification model using an EmbeddingBag + linear layer.\n",
    "\n",
    "    Architecture:\n",
    "      - EmbeddingBag(vocab_size, embed_dim): computes a single embedding per\n",
    "        example by summing/averaging token embeddings using `offsets`.\n",
    "      - Linear(embed_dim, num_class): projects the pooled embedding to class logits.\n",
    "\n",
    "    Why EmbeddingBag?\n",
    "      - EmbeddingBag avoids explicit padding by representing each example as a\n",
    "        contiguous span in a flattened token tensor together with start\n",
    "        `offsets`. This is memory-efficient and fast for short-to-medium texts.\n",
    "\n",
    "    Forward inputs (expected):\n",
    "      - text: 1D LongTensor with concatenated token IDs for a batch\n",
    "      - offsets: 1D LongTensor of start positions for each example in `text`\n",
    "\n",
    "    Example (runnable):\n",
    "      # Suppose we have 2 examples with token ID sequences:\n",
    "      #   e0 -> [1,2,3]  (length 3)\n",
    "      #   e1 -> [4,5]    (length 2)\n",
    "      # Flatten: text = [1,2,3,4,5], offsets = [0,3]\n",
    "      # The model will compute an embedding for e0 from text[0:3], for e1 from\n",
    "      # text[3:5], and then produce logits of shape (batch_size, num_class).\n",
    "\n",
    "    Note: EmbeddingBag's default mode is 'mean' for reduction in newer PyTorch\n",
    "    versions; when using this model for sum/mean differences may affect scale of\n",
    "    gradients and learning rates.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        # EmbeddingBag creates embeddings for variable-length examples using\n",
    "        # a flattened `text` tensor + `offsets` to group tokens per sample.\n",
    "        # Set `sparse=True` to enable sparse gradients if using very large\n",
    "        # vocabularies and sparse optimizers (e.g., SparseAdam). For small\n",
    "        # experiments it's fine to leave sparse=False.\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "\n",
    "        # Linear layer maps pooled embeddings to logits for each class\n",
    "        # fc = fully-connected output layer producing logits\n",
    "        # \"fc\" is a common short name for \"fully connected\" (a.k.a. dense) layer.\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights to small uniform values for stable starts.\"\"\"\n",
    "        initrange = 0.5\n",
    "        # Embedding weight initialization\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        # Linear layer weight and bias initialization\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "          text: 1D LongTensor with concatenated token IDs, shape (total_tokens,)\n",
    "          offsets: 1D LongTensor with start indices for each example, shape (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "          logits: Tensor of shape (batch_size, num_class)\n",
    "        \"\"\"\n",
    "        # embedding: shape (batch_size, embed_dim)\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        # logits: shape (batch_size, num_class)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "# Instantiate model for training code below\n",
    "num_class = 4\n",
    "embed_dim = 64\n",
    "\n",
    "# Small runnable demonstration (uses a tiny synthetic batch so it runs quickly)\n",
    "# Build a tiny synthetic batch with two examples to demonstrate shapes\n",
    "# Note: uses `num_tokens` and `device` defined earlier in the notebook.\n",
    "try:\n",
    "    # Create a minimal synthetic batch of token IDs within vocabulary range\n",
    "    example_text = torch.randint(low=0, high=max(2, num_tokens), size=(7,), dtype=torch.long)\n",
    "    # Example offsets for two samples: first 4 tokens, next 3 tokens\n",
    "    example_offsets = torch.tensor([0, 4], dtype=torch.long)\n",
    "    # Run a forward pass and print shapes\n",
    "    example_logits = TextClassificationModel(num_tokens, embed_dim, num_class)(example_text.to(device), example_offsets.to(device))\n",
    "    print('\\nDemo forward pass logits shape:', example_logits.shape)\n",
    "except Exception as e:\n",
    "    # If something fails (e.g., in a partially-initialized kernel), show helpful message\n",
    "    print('\\nDemo forward pass skipped (reason):', e)\n",
    "\n",
    "\n",
    "model = TextClassificationModel(num_tokens, embed_dim, num_class).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3875c62",
   "metadata": {},
   "source": [
    "Define the loss function, optimizer, and the training/validation loops. The training loop iterates over batches, computes loss and backpropagates, while the evaluation loop measures accuracy on a held-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c062bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation utilities\n",
    "\n",
    "# Loss function: CrossEntropyLoss expects raw logits (no softmax) and\n",
    "# integer class labels (0..num_classes-1). It combines LogSoftmax + NLLLoss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: SGD here. The learning rate (lr=5.0) is intentionally large in\n",
    "# the tutorial to illustrate quick learning on this small model — in practice\n",
    "# you will usually use much smaller values (e.g., 0.01 or 0.001) or a\n",
    "# different optimizer such as Adam.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5.0)\n",
    "\n",
    "# Progress bars: `tqdm` wraps iterables (like DataLoader) to show progress.\n",
    "# In Jupyter notebooks you may prefer `from tqdm.notebook import tqdm` for\n",
    "# richer progress-bar rendering.\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device=device, grad_clip=None, show_progress=False):\n",
    "    \"\"\"Run one epoch of training and return accuracy.\n",
    "\n",
    "    Beginner-level summary of what happens in one loop:\n",
    "      1. Move inputs to the chosen `device` (CPU or GPU).\n",
    "      2. Run the model to get raw outputs (logits).\n",
    "      3. Compute the loss by comparing logits to true labels.\n",
    "      4. Backpropagate gradients (loss.backward()).\n",
    "      5. Update model weights with optimizer.step().\n",
    "      6. Track accuracy and optionally show progress.\n",
    "\n",
    "    Parameters:\n",
    "      - model: your neural network (nn.Module)\n",
    "      - dataloader: yields (text, offsets, labels) per batch\n",
    "      - optimizer: updates weights (e.g., SGD)\n",
    "      - criterion: loss function (e.g., CrossEntropyLoss)\n",
    "      - device: device to use (string/torch.device). If your collate_fn\n",
    "                already moves tensors to device then these moves are safe\n",
    "                and redundant; otherwise they ensure correct placement.\n",
    "      - grad_clip: optional float to clip gradients after backward()\n",
    "      - show_progress: if True, wraps the dataloader with a progress bar\n",
    "\n",
    "    Returns:\n",
    "      - train_accuracy: fraction of correctly predicted samples for the epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Put the model in training mode so layers like dropout/batchnorm behave correctly\n",
    "    model.train()\n",
    "\n",
    "    total_acc, total_count = 0, 0   # counters for accuracy\n",
    "    total_loss = 0.0                # accumulate loss for a summary\n",
    "    log_interval = 500              # how often to print a short status\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Wrap the dataloader with a progress bar if requested\n",
    "    data_iter = tqdm(dataloader, desc='train') if show_progress else dataloader\n",
    "\n",
    "    for idx, (text, offsets, labels) in enumerate(data_iter):\n",
    "        # Move tensors to device. If they are already on `device` this\n",
    "        # operation is cheap and safe (it will not copy unnecessarily).\n",
    "        text = text.to(device)\n",
    "        offsets = offsets.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 1) Zero gradients from previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2) Forward pass: compute model outputs (logits)\n",
    "        output = model(text, offsets)\n",
    "\n",
    "        # 3) Compute loss comparing logits with true integer labels\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # 4) Backprop: compute gradients of loss w.r.t. model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Optional: clip gradients to avoid exploding gradients (useful\n",
    "        # when using a large learning rate or unstable training).\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        # 5) Update model parameters\n",
    "        # the optimizer holds references to the model's parameter tensors and updates them in-place during optimizer.step() using the gradients populated by loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6) Compute simple accuracy for this batch and accumulate\n",
    "        # `output.argmax(1)` selects the predicted class per example.\n",
    "        pred = output.argmax(1)\n",
    "        batch_correct = (pred == labels).sum().item()\n",
    "        total_acc += batch_correct\n",
    "        total_count += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Periodic logging: helpful when you want to watch a long epoch.\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = total_loss / (idx + 1)\n",
    "            avg_acc = total_acc / total_count if total_count > 0 else 0.0\n",
    "            # This print shows a quick summary without plotting: index, avg loss, avg accuracy, elapsed time\n",
    "            print(f\"[train] idx={idx+1} avg_loss={avg_loss:.4f} avg_acc={avg_acc:.4f} elapsed={elapsed:.1f}s\")\n",
    "\n",
    "    # Final epoch-level metrics\n",
    "    train_accuracy = total_acc / total_count if total_count > 0 else 0.0\n",
    "    final_avg_loss = total_loss / (idx + 1) if (idx + 1) > 0 else 0.0\n",
    "    print(f\"Epoch training summary -> avg_loss: {final_avg_loss:.4f}, accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    return train_accuracy\n",
    "\n",
    "\n",
    "# Evaluate the model on a validation / test set\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device=device, show_progress=False):\n",
    "    \"\"\"Run evaluation and return accuracy.\n",
    "\n",
    "    Beginner-level notes:\n",
    "      - Use `model.eval()` to turn off training-only behavior (dropout, batchnorm updates).\n",
    "      - Wrap evaluation in `torch.no_grad()` to disable gradient computations\n",
    "        which saves memory and speeds up the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    data_iter = tqdm(dataloader, desc='eval') if show_progress else dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, offsets, labels) in enumerate(data_iter):\n",
    "            # Move to device (same reasoning as in train loop)\n",
    "            text = text.to(device)\n",
    "            offsets = offsets.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass only — no backward or optimizer steps here\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            pred = output.argmax(1)\n",
    "            total_acc += (pred == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    eval_accuracy = total_acc / total_count if total_count > 0 else 0.0\n",
    "    avg_loss = total_loss / (idx + 1) if (idx + 1) > 0 else 0.0\n",
    "    print(f\"Evaluation summary -> avg_loss: {avg_loss:.4f}, accuracy: {eval_accuracy:.4f}\")\n",
    "    return eval_accuracy\n",
    "\n",
    "\n",
    "# --- Small commented examples for new users ---\n",
    "#\n",
    "# 1) Run one quick epoch with a visible progress bar and gradient clipping:\n",
    "# train_acc = train_epoch(model, train_dataloader, optimizer, criterion,\n",
    "#                         device=device, grad_clip=1.0, show_progress=True)\n",
    "#\n",
    "# 2) Evaluate after training:\n",
    "# test_acc = evaluate(model, test_dataloader, criterion, device=device, show_progress=True)\n",
    "#\n",
    "# 3) If your `collate_batch` already sends tensors to GPU, you can call the\n",
    "#    functions without passing `device` and they will work correctly.\n",
    "#\n",
    "# 4) If you want to track loss history for plotting, collect `loss.item()` in\n",
    "#    a list inside the loop or modify `train_epoch` to return (accuracy, avg_loss).\n",
    "#\n",
    "# Quick tip: when training on GPU, set DataLoader(..., pin_memory=True) and\n",
    "# use num_workers>0 to speed up host->device transfers and data loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4cd8e",
   "metadata": {},
   "source": [
    "Run a short training cycle using the training and evaluation utilities defined above. This cell performs a quick sanity run for a small number of epochs to verify the pipeline is working end-to-end.\n",
    "\n",
    "On CPU each epoch takes about 20s.  On GPU each epoch takes about 10s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] idx=500 avg_loss=0.7172 avg_acc=0.7164 elapsed=3.1s\n",
      "[train] idx=1000 avg_loss=0.5595 avg_acc=0.7899 elapsed=5.5s\n",
      "[train] idx=1000 avg_loss=0.5595 avg_acc=0.7899 elapsed=5.5s\n",
      "[train] idx=1500 avg_loss=0.4901 avg_acc=0.8205 elapsed=8.1s\n",
      "[train] idx=1500 avg_loss=0.4901 avg_acc=0.8205 elapsed=8.1s\n",
      "Epoch training summary -> avg_loss: 0.4583, accuracy: 0.8343\n",
      "Epoch training summary -> avg_loss: 0.4583, accuracy: 0.8343\n",
      "Evaluation summary -> avg_loss: 0.3353, accuracy: 0.8909\n",
      "Epoch: 1, Train acc: 0.8343, Test acc: 0.8909\n",
      "Evaluation summary -> avg_loss: 0.3353, accuracy: 0.8909\n",
      "Epoch: 1, Train acc: 0.8343, Test acc: 0.8909\n",
      "[train] idx=500 avg_loss=0.2886 avg_acc=0.9030 elapsed=2.4s\n",
      "[train] idx=500 avg_loss=0.2886 avg_acc=0.9030 elapsed=2.4s\n",
      "[train] idx=1000 avg_loss=0.2879 avg_acc=0.9038 elapsed=4.7s\n",
      "[train] idx=1000 avg_loss=0.2879 avg_acc=0.9038 elapsed=4.7s\n",
      "[train] idx=1500 avg_loss=0.2833 avg_acc=0.9047 elapsed=7.1s\n",
      "[train] idx=1500 avg_loss=0.2833 avg_acc=0.9047 elapsed=7.1s\n",
      "Epoch training summary -> avg_loss: 0.2821, accuracy: 0.9050\n",
      "Epoch training summary -> avg_loss: 0.2821, accuracy: 0.9050\n",
      "Evaluation summary -> avg_loss: 0.2975, accuracy: 0.9012\n",
      "Epoch: 2, Train acc: 0.9050, Test acc: 0.9012\n",
      "Done training (short run).\n",
      "Evaluation summary -> avg_loss: 0.2975, accuracy: 0.9012\n",
      "Epoch: 2, Train acc: 0.9050, Test acc: 0.9012\n",
      "Done training (short run).\n"
     ]
    }
   ],
   "source": [
    "# Run a longer training loop (10 epochs) and plot train/test accuracy\n",
    "\n",
    "# Ensure we start from a newly initialized model per the exercise instruction.\n",
    "# Recreate model and optimizer so we don't continue training a previously\n",
    "# trained instance.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Re-initialize model and optimizer for a fresh training run\n",
    "model = TextClassificationModel(num_tokens, embed_dim, num_class).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5.0)\n",
    "\n",
    "n_epochs = 10\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{n_epochs} ---\")\n",
    "    # Train for one epoch (uses the train_epoch function defined above)\n",
    "    train_acc = train_epoch(model, train_dataloader, optimizer, criterion,\n",
    "                            device=device, grad_clip=None, show_progress=False)\n",
    "\n",
    "    # Evaluate on the held-out test set\n",
    "    test_acc = evaluate(model, test_dataloader, criterion, device=device, show_progress=False)\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch} summary -> train_acc: {train_acc:.4f}, test_acc: {test_acc:.4f}\")\n",
    "\n",
    "# Plot training and test accuracy vs epoch\n",
    "plt.figure(figsize=(8,5))\n",
    "epochs = np.arange(1, n_epochs + 1)\n",
    "plt.plot(epochs, train_accs, marker='o', label='Train accuracy')\n",
    "plt.plot(epochs, test_accs, marker='o', label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "plt.xticks(epochs)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Mark the epoch where test accuracy peaked\n",
    "peak_test_epoch = int(np.argmax(test_accs)) + 1\n",
    "plt.axvline(peak_test_epoch, color='red', linestyle='--', alpha=0.6)\n",
    "plt.annotate(f'Peak test @ epoch {peak_test_epoch}', xy=(peak_test_epoch, test_accs[peak_test_epoch-1]),\n",
    "             xytext=(peak_test_epoch, max(test_accs) - 0.05),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Simple heuristic to indicate potential overfitting:\n",
    "# - If test accuracy peaks earlier than final epoch and then decreases afterwards,\n",
    "#   that suggests overfitting may begin after the peak.\n",
    "# - Also report the epoch with the largest train/test gap.\n",
    "\n",
    "peak_test = peak_test_epoch\n",
    "if peak_test < n_epochs and test_accs[-1] < test_accs[peak_test - 1]:\n",
    "    print(f\"\\nWarning: test accuracy peaked at epoch {peak_test} and decreased by the final epoch -> possible overfitting starting after epoch {peak_test}.\")\n",
    "else:\n",
    "    print(f\"\\nTest accuracy peak at epoch {peak_test}. No clear post-peak decrease detected by this heuristic.\")\n",
    "\n",
    "# Report the epoch with the largest train-test gap (may indicate overfitting)\n",
    "gaps = [t - v for t, v in zip(train_accs, test_accs)]\n",
    "epoch_max_gap = int(np.argmax(gaps)) + 1\n",
    "print(f\"Epoch with largest train-test gap: {epoch_max_gap} (gap={gaps[epoch_max_gap-1]:.4f})\")\n",
    "\n",
    "# End of training & analysis cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c95f33",
   "metadata": {},
   "source": [
    "Notes / next steps\n",
    "\n",
    "- This example builds a small vocabulary and trains a simple EmbeddingBag model for AG_NEWS.\n",
    "- **Exercise:** modify the training loop to run for 10 epochs and plot the training and test accuracy versus epoch. Is there evidence of overfitting? At which epoch?  *Be sure to train a newly initialized model!*\n",
    "- **Optional exercise:** read up on optimizers and introduce the Adam optimizer into the pipeline. See for example [https://pythonguides.com/adam-optimizer-pytorch/](https://pythonguides.com/adam-optimizer-pytorch/).  Compare the results trained using Adam with the default SGD optimizer.\n",
    "\n",
    "Additional suggestions for better performance:\n",
    "  - Use pretrained embeddings or larger embed_dim.\n",
    "  - Use regularization, LR scheduling, or a different optimizer.\n",
    "  - Use text preprocessing: lowercasing, stopword filtering, subword tokenization.\n",
    "  - Explore transformer-based models with Hugging Face for state-of-the-art performance (session 3!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282aef2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03502cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
